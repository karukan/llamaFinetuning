{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a07f6d73-8bbb-4832-b0cd-a3f30aba009d",
      "metadata": {
        "id": "a07f6d73-8bbb-4832-b0cd-a3f30aba009d"
      },
      "source": [
        "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/C3669C-2025-02/blob/main/Lesson_10/L10_Colab_Run_2025-09-27.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae6ff1dc-2734-497d-be36-279f2dc76beb",
      "metadata": {
        "id": "ae6ff1dc-2734-497d-be36-279f2dc76beb"
      },
      "source": [
        "# Setup and Installation\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" width=\"20%\" height=\"auto\"/>\n",
        "\n",
        "You **MUST** run this Jupyter notebook at Google Colab.  We will using **unsloth** library. **Unsloth** makes finetuning large language models like Llama-3 2X faster and using 70% less memory with no degradation in accuracy.\n",
        "\n",
        "The following Python code was modified/adapted from Unsloth.\n",
        "\n",
        "### References\n",
        "- [github/unsloth](https://github.com/unslothai/unsloth)\n",
        "- [Unsloth documentation](https://docs.unsloth.ai/)\n",
        "- [huggingface/unsloth](https://huggingface.co/unsloth)\n",
        "- [How to fine-tune an LLM](https://sausheong.com/how-to-fine-tune-an-llm-dc1c7376eb2b)\n",
        "- [4 Open Source Libraries Speed Up LLM Fine-Tuning](https://www.linkedin.com/posts/sumanth077_fine-tuning-massive-llms-used-to-be-painfully-activity-7349701022719623168-zKAY)\n",
        "- [Best frameworks for fine-tuning LLMs in 2025](https://modal.com/blog/fine-tuning-llms)\n",
        "\n",
        "### Versions\n",
        "- unsloth (2025.9.9)\n",
        "- unsloth_zoo (2025.9.12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35db9108-5cd7-4d05-ab39-6ca01fa370d2",
      "metadata": {
        "id": "35db9108-5cd7-4d05-ab39-6ca01fa370d2"
      },
      "source": [
        "### Google Colab\n",
        "- At Google Colab, Go to `Runtime` > `Change runtime type` and select `T4 GPU`. Or if you are lucky, choose `A100 GPU`.\n",
        "- Using T4 GPU is free with limited access time\n",
        "- If you have Colab+ subscription, you can select A100 GPU.\n",
        "\n",
        "<img src=\"https://github.com/koayst-rplesson/C3669C-2025-02/blob/main/Lesson_10/colab-01.png?raw=1\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f63cfe5f-e947-458c-82f6-d6a2ca4fda98",
      "metadata": {
        "id": "f63cfe5f-e947-458c-82f6-d6a2ca4fda98"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install unsloth\n",
        "\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e23847-01ec-41ad-9879-153f503cc11a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16e23847-01ec-41ad-9879-153f503cc11a",
        "outputId": "a886255b-fd94-40f1-dfae-99d0ccc6ede2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None          # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True   # Use 4bit quantization to reduce memory usage. Can be False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e271b221-9742-42ed-9192-69db51006784",
      "metadata": {
        "id": "e271b221-9742-42ed-9192-69db51006784"
      },
      "outputs": [],
      "source": [
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5405f52d-95a7-4ba2-8d9f-41d7adbb5b5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "adb14ab923834d2086db3c47168b7a82",
            "e29acfbb113948dca1e49f4c567fab35",
            "7aba3f8363984fe797da0819a30ec629",
            "9cbddbb3f31049888db36497a415b369",
            "2493b03e30b04eb48909a1bf19a0b2f7",
            "66b0d8835be64e138f29006628d3f298",
            "71018566bfeb4714b1f3a02c34699891",
            "cec4a2addb154079945bb42b359b996c",
            "8d21069fe17643d29c2e9f42907fd190",
            "8cb522ff83784ea48952f4d79401479b",
            "2c0d92526f0349239369715001d5ffe0",
            "88be6a22b3cd45d5832bcb0b471e04f6",
            "b9979c352a2548919ae861a6d402b59e",
            "b5be4cca0b304e6d8af40ae3ab655342",
            "c521468850a74a6e9235f3a97dd3f423",
            "41910cfd20cd4ae889c7695e10e23c39",
            "e249359adc2941728bb78e1f964e12b6",
            "e7a4b489caa5414aa189ad934a061d1d",
            "bac35c24d2dc4e988b4a50853b190e16",
            "603e75c8b7354287aee480f91eacd336",
            "8a45438f66054ad8b54fd5b3e5dc44ee",
            "332d014865b246f099178bd75033edd3",
            "d84c32a21e37495c8a4b8fe417cd90be",
            "0f4fe196c1394ba8b3fefb0504ff4c2c",
            "4dbe10eccc994b64a3917e62150b27c7",
            "ea39d0f360034e62863da1e85288ffad",
            "f0c7d05275734ea4a4be3f1cf939e367",
            "7e9017344bfb4e818d91fdb944da6c29",
            "87f4cbb688fe4d669acdbfd6ebe2b79d",
            "e71527f1495c488f9b0e43a5f4fe979f",
            "7f39274b26ec4ddfaf96aef87fc762d9",
            "58dd8ddf71044d2bb6e1252ee5f66bc3",
            "4b2c1a16c7f64f7f85ac2cbd3e99417a",
            "3905ed945a774b7fa18a304eca2e55a3",
            "61b7490835b849adbd832ac9d4daf83e",
            "c2122d69feaf4778a4f6ce62e6ad4da3",
            "6fbdf999797a46f28fdf4aca4b35214a",
            "2a99e9804f184666a8b6e9648f9382c7",
            "cfdb41f871c14f34a3b91ca09c721377",
            "2b1e5fb14b1a42278a5b9803dc7a170a",
            "9a965494d0c746b1839bd2ed377931d9",
            "6844f24e2a374ba7ac1b21d1c6794525",
            "2c0b02ceae714d84b64fd3f3116a1586",
            "11a53258b10f44f6833bd5a8d5cb61e3",
            "78d341ca4097457bb0cddd78e717acf7",
            "0ee1076f59c942f796f41d236ce87bb1",
            "2c3355c5a5074f7aa4f63f048ac895b4",
            "021894a7e897468c8f3b064f21f88931",
            "e91c846a0c574312878d012d3ebeef7e",
            "3a04e4cad16648c992d122c62a368902",
            "0207e99eb83743438ffdd82f33c2533d",
            "c803c79bad0a4a6baefe3b2fc5f1e9c5",
            "29c0bd2778d347d38ade89c523ee1d5f",
            "2ca351e016c049f09a2501ac0c4957f5",
            "fc3c7a174e164bd7bb990a9be1a5e8ec"
          ]
        },
        "id": "5405f52d-95a7-4ba2-8d9f-41d7adbb5b5d",
        "outputId": "102da938-f999-4bfc-81aa-ba44f1afdb3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.9.9: Fast Llama patching. Transformers: 4.56.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adb14ab923834d2086db3c47168b7a82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88be6a22b3cd45d5832bcb0b471e04f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d84c32a21e37495c8a4b8fe417cd90be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3905ed945a774b7fa18a304eca2e55a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78d341ca4097457bb0cddd78e717acf7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# if you happened to use one of the gated models (for example: meta/llama 3.2), you need to go to huggingface to apply for permission to access.\n",
        "# Possible error message looks like this:\n",
        "#    Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list.\n",
        "#    Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.`\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e1e3fe1-3c9b-4d4d-b49f-4007107aaf53",
      "metadata": {
        "id": "8e1e3fe1-3c9b-4d4d-b49f-4007107aaf53"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02bc4fb5-d612-4851-962c-fb95261f4c19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02bc4fb5-d612-4851-962c-fb95261f4c19",
        "outputId": "82aefbed-4db4-4d21-b7d6-1e2b0a0b990e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.9.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "\n",
        "    target_modules = [\"q_proj\",\n",
        "                      \"k_proj\",\n",
        "                      \"v_proj\",\n",
        "                      \"o_proj\",\n",
        "                      \"gate_proj\",\n",
        "                      \"up_proj\",\n",
        "                      \"down_proj\",],\n",
        "\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcb6982f-4090-4995-a95e-cdcc47c8ce36",
      "metadata": {
        "id": "dcb6982f-4090-4995-a95e-cdcc47c8ce36"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Preparation\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e822a697-6779-4ed3-938c-98174cf120ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "29383225b65543f1ae81fe53a6892098",
            "e986d1ceb7d84e0e99f9e3346aa9a67c",
            "4e4a58027a354c09a7b8b3d4f9c5469e",
            "6fcd6041cd1f436ea4486d1f159ba6fc",
            "ac6029dde7db4a259d75800838248b54",
            "7fc9ff4506694f3fa06b8d5178edca3e",
            "c842c714bdd34466a333c428517ffa54",
            "fa048ded84ee47eaa2503e36e5e16ab7",
            "f59e87e4c5a04c6090ee3a7fce549807",
            "daf8ddfa3f2e47dfa25caad7a4ad5e74",
            "3670060316ea4feb98c13ffe27a0a816",
            "88a63b0bcbd342369661639ab8fb449c",
            "6e68a49ba8a1482c98dcb3cdcca9ee33",
            "83457daf51bb4b0b89c67ff9dbbc146f",
            "1b452faf65224462a37837a23ab56699",
            "4235744cd89f4f889b3951939ba90b76",
            "460d58bd25964d0683439abd5f07ef42",
            "9a7ed5e0288144b19222be3dfe44b88d",
            "d208f1f5863e44648bdc805ea25f4bd0",
            "80a131ce7e5848de992ccd390156fc24",
            "3fb80abded9b4bef95dadc89bf76c434",
            "3f3e33dac1874dd0b58fd3ac36202955",
            "ef5fc187e36843adb0ba2aeba38fc2e8",
            "5c0ad0df90e54d0c9e7872d08d338958",
            "3a70321823a04c119db4d045f4982492",
            "597d1502f02143eba5852f50c5900d37",
            "09ff34966bfc48c194b13a7e0f0cfa59",
            "ebc9e784bcd24ce587e0aa0adde8a650",
            "e9699f0e23964eec8de1b83f5c044c5d",
            "012dbcf161634e40acb80ed512cffa53",
            "bfaf8274a8e245f295c367f896225c5e",
            "93d292271d0a4d0ba8d87654964db4c4",
            "07c33fb9fbc04671a1cc03eb82a0b0d5",
            "8bccce1514f34b8a8a1b857f28068c77",
            "9d3ab27cc2ae4b4cba10e7d88f7c1428",
            "bffd06b08aa1483481ab25fbc095643a",
            "caaea18367624635b0a8ab29e0b2d75b",
            "021a34e3a70b4de2aa410a1777fc4755",
            "b24d0be4f89947ad8613ee59d561ba29",
            "9f516bc6d13345d5ab83b1a8f723eb69",
            "eb2ad0fdda714f4487b06aaf7cc3a491",
            "695edebff1dd4342b59f5a418cc384cb",
            "a9d9c6658173485cb86f0d1743e88249",
            "61c1cf627c4342009509098970901340"
          ]
        },
        "id": "e822a697-6779-4ed3-938c-98174cf120ac",
        "outputId": "b30e12d3-6034-4471-9745-d71bcedf2a3d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29383225b65543f1ae81fe53a6892098",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88a63b0bcbd342369661639ab8fb449c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "alpaca_data_cleaned.json:   0%|          | 0.00/44.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef5fc187e36843adb0ba2aeba38fc2e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bccce1514f34b8a8a1b857f28068c77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903ac708-f84e-4595-8b65-c2c5baf3eef0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "903ac708-f84e-4595-8b65-c2c5baf3eef0",
        "outputId": "fc06e303-faa2-438c-a0d6-f53f21305a42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['output', 'input', 'instruction', 'text'],\n",
            "    num_rows: 51760\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3325f3d-844d-4d8b-ae43-069b59f5d904",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3325f3d-844d-4d8b-ae43-069b59f5d904",
        "outputId": "4ea75530-b8ea-4e0b-fe5f-77219c58e004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
            "\n",
            "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
            "\n",
            "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.\n",
            "\n",
            "Input:\n",
            "\n",
            "\n",
            "Instruction:\n",
            "Give three tips for staying healthy.\n",
            "\n",
            "Text:\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Give three tips for staying healthy.\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
            "\n",
            "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
            "\n",
            "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|end_of_text|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Output:\\n{dataset[0]['output']}\\n\")\n",
        "print(f\"Input:\\n{dataset[0]['input']}\\n\")\n",
        "print(f\"Instruction:\\n{dataset[0]['instruction']}\\n\")\n",
        "print(f\"Text:\\n{dataset[0]['text']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a15d7ee-cea0-4272-b17d-d834126277ef",
      "metadata": {
        "id": "7a15d7ee-cea0-4272-b17d-d834126277ef"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa217556-7984-48cb-b122-5208878bb6ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "af90c8dca4a24afbae51b19057c55477",
            "0eef07c64f4741678e0587f9dc62454b",
            "2f4e251f190b472baa134d2f6111e115",
            "111b8e087d1e41e4a25abe56e289a26e",
            "8240db8129e841dab607c0930351571b",
            "cb8e561818c24572ac151a9e7f1d01a8",
            "1628dbeb21a94468ac1b431d2d8ffd01",
            "9e42f707d0bb4092a2bf409e156f3651",
            "6d5cc035ad8745718e7ac5606d640bf2",
            "c79f20bc342045af91f649cce5ace93f",
            "dd4cf1082767445c9fa7dc4b63f4bdbe"
          ]
        },
        "id": "aa217556-7984-48cb-b122-5208878bb6ca",
        "outputId": "e5b3c6ba-4863-4922-9795-76e3b3d5572b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af90c8dca4a24afbae51b19057c55477",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/51760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c3f0847-7d17-46b3-b96b-f454b6c1f1a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c3f0847-7d17-46b3-b96b-f454b6c1f1a0",
        "outputId": "4f6fdc35-ab29-4aa7-bce5-164a8ba43a04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "6.881 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebbb4f94-8023-452e-98b5-2098cc51bc36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ebbb4f94-8023-452e-98b5-2098cc51bc36",
        "outputId": "535c6c1e-a6f7-47d6-cfb4-71d0fc5ce4a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 51,760 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 04:23, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.742500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.402600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.883100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.533900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.556100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.383200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.382600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.429100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.057200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.278300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.104300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.135900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.869900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.784600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.060900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.128900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.171000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.737200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.022100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.015400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.870200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.837000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.035000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.844000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.901200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.829300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.190200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.803900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.695700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.927500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.848000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.790500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.914200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.979100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.603300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.889000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.044000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.787000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.816500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.909900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.922300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.951400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.770100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.043900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.814400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.100700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.969600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.080400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.086600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.926800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.025700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.924400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.873500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.813200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.001500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.846500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.829700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 4min 18s, sys: 1.33 s, total: 4min 19s\n",
            "Wall time: 4min 38s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ee1f4a-98d6-493e-a437-bc0dcffaa18c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0ee1f4a-98d6-493e-a437-bc0dcffaa18c",
        "outputId": "2a19caca-8e27-4a3a-fa4d-acd028bc0520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "275.2037 seconds used for training.\n",
            "4.59 minutes used for training.\n",
            "Peak reserved memory = 6.881 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 46.679 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6118073-c1e6-465a-a116-cc39b92f4e3c",
      "metadata": {
        "id": "c6118073-c1e6-465a-a116-cc39b92f4e3c"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b4c5d5-5ee0-45bb-9865-468a8b4dd8c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27b4c5d5-5ee0-45bb-9865-468a8b4dd8c8",
        "outputId": "7de8ca25-0d99-4f79-c886-fa0c49f40af7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nContinue the fibonnaci sequence.\\n\\n### Input:\\n1, 1, 2, 3, 5, 8\\n\\n### Response:\\n13, 21, 34, 55, 89, 144, 233, 377, 610, 987<|end_of_text|>']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "451dc78c-ed93-4979-9b4c-9bfce9ee6610",
      "metadata": {
        "id": "451dc78c-ed93-4979-9b4c-9bfce9ee6610"
      },
      "source": [
        "You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "911e502e-1758-46ee-9422-8a0fffdc2ce2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "911e502e-1758-46ee-9422-8a0fffdc2ce2",
        "outputId": "c4a9e0c5-7ad1-474e-cf54-8be10616d1d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "13, 21, 34, 55, 89, 144<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output/response - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4193f44d-98da-4825-b263-e260d9519e83",
      "metadata": {
        "id": "4193f44d-98da-4825-b263-e260d9519e83"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce2bc6f-c82d-4847-b2b9-179987495256",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ce2bc6f-c82d-4847-b2b9-179987495256",
        "outputId": "2e4c5940-3d67-43d5-8d99-894f72ee4164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# connect to google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f456cc49-9380-4dd8-b650-a3ba2d1773c6",
      "metadata": {
        "id": "f456cc49-9380-4dd8-b650-a3ba2d1773c6"
      },
      "outputs": [],
      "source": [
        "path_to_saved_model = \"/content/drive/MyDrive/Colab Notebooks/c3669c/L06/lora_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71435773-e19e-45b3-bd4e-8fee9a94359f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71435773-e19e-45b3-bd4e-8fee9a94359f",
        "outputId": "6d9928bc-cc25-4d36-f841-88d1dc74fbe2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab Notebooks/c3669c/L06/lora_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/c3669c/L06/lora_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/c3669c/L06/lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(path_to_saved_model) # Local saving\n",
        "tokenizer.save_pretrained(path_to_saved_model)\n",
        "\n",
        "# you can push to huggingface if you have an account\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe03aee-35d3-4249-9aca-38a2ada511da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fe03aee-35d3-4249-9aca-38a2ada511da",
        "outputId": "9c3b4f74-ef65-4580-fe57-b3480692863e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 180763\n",
            "-rw------- 1 root root      1088 Sep 27 11:49 adapter_config.json\n",
            "-rw------- 1 root root 167832240 Sep 27 11:49 adapter_model.safetensors\n",
            "-rw------- 1 root root      5260 Sep 27 11:49 README.md\n",
            "-rw------- 1 root root       459 Sep 27 11:49 special_tokens_map.json\n",
            "-rw------- 1 root root     50647 Sep 27 11:49 tokenizer_config.json\n",
            "-rw------- 1 root root  17209920 Sep 27 11:49 tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "# check the files are save to directory lora_model\n",
        "\n",
        "!ls -al \"/content/drive/MyDrive/Colab Notebooks/c3669c/L06/lora_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8afc5ee-1c94-4e01-84f1-63a932e8fd9d",
      "metadata": {
        "id": "c8afc5ee-1c94-4e01-84f1-63a932e8fd9d"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2c67e5-d0cc-407d-8ece-4fcc50fbe617",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae2c67e5-d0cc-407d-8ece-4fcc50fbe617",
        "outputId": "8cc84ff3-9820-4e37-dcf5-f6849bb29acb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.9.9: Fast Llama patching. Transformers: 4.56.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = path_to_saved_model, # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f6b1c49-0668-4787-89e7-ec7396aead14",
      "metadata": {
        "id": "6f6b1c49-0668-4787-89e7-ec7396aead14"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = You MUST copy from above!\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4179207-85ab-41ba-8029-0b11ed3252ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4179207-85ab-41ba-8029-0b11ed3252ce",
        "outputId": "3d2a7b42-77b4-4077-cbad-34dfd6a06c94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is a famous tall tower in Paris?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The Eiffel Tower is a famous tall tower in Paris. It was built in 1889 as the entrance arch for the World's Fair and is now one of the most iconic landmarks in the world. It is 324 meters (1,063 feet) tall and has three levels, with restaurants and observation decks at each level.<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"What is a famous tall tower in Paris?\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "808bec63-ca41-409a-88a5-e49ac0740336",
      "metadata": {
        "id": "808bec63-ca41-409a-88a5-e49ac0740336"
      },
      "source": [
        "### Cleanup\n",
        "\n",
        "If you don't need the models anymore, remember to go to `path_to_saved_model` and delete the directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d03441",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
