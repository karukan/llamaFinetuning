{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84569354",
   "metadata": {},
   "source": [
    "# V1.1 Llama 3.2  — MetaMathQA fine-tuning (Colab-ready)\n",
    "This notebook prepares MetaMathQA data and demonstrates a PEFT/LoRA fine-tuning workflow suitable for Llama-family causal models. Run in Google Colab with a GPU runtime for best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c95182",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "This notebook contains: (1) environment and Colab quickstart, (2) data preparation for MetaMathQA, (3) example training using Hugging Face Transformers + PEFT (LoRA), and (4) evaluation examples.\n",
    "\n",
    "Intended usage: open in Colab (Runtime → Change runtime type → GPU), run the setup cell, prepare data, then run the training cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24fe75",
   "metadata": {},
   "source": [
    "## 2. Environment & Colab quickstart\n",
    "If you run this notebook locally without a CUDA GPU, training will fail or be extremely slow — prefer Colab or other GPU hosts.\n",
    "\n",
    "Open in Colab: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/karukan/llamaFinetuning/blob/main/llama3.2-finetune-metamathqa.ipynb)\n",
    "\n",
    "Quick steps: set Runtime→Change runtime type→GPU, run the setup cell (mount Drive if you want checkpoints persisted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('CUDA available?', torch.cuda.is_available())\n",
    "print('CUDA devices:', torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print('Current device name:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc993b1a",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "We load `meta-math/MetaMathQA` via the `datasets` library, clean the text, and format prompt-completion pairs. The target format used below is a JSONL where each line is {\"prompt\":..., \"completion\":...} suitable for many LLM fine-tuning tools.\n",
    "Option 2 justification: `MetaMathQA` is chosen because it contains math reasoning Q&A examples that can help the model specialize in formal mathematical problem phrasing and solution generation—useful for benchmarking math reasoning improvements after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json, os\n",
    "\n",
    "# Load dataset from the hub. If you have it locally, adapt the path.\n",
    "dataset_name = 'meta-math/MetaMathQA'\n",
    "print('Loading dataset:', dataset_name)\n",
    "try:\n",
    "    ds = load_dataset(dataset_name)\n",
    "except Exception as e:\n",
    "    print('Failed to load directly. Check network/access or replace with local path. Error:', e)\n",
    "    ds = None\n",
    "\n",
    "# Inspect if loaded\n",
    "if ds is not None:\n",
    "    print(ds)\n",
    "    # show a few examples (train split may be named 'train')\n",
    "    for k in ds.keys():\n",
    "        print('Split', k, '->', ds[k].num_rows)\n",
    "    print('Example row (first train if exists):')\n",
    "    split = list(ds.keys())[0]\n",
    "    print(ds[split][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc7289",
   "metadata": {},
   "source": [
    "### 4b. Data cleaning and formatting helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209afa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(s):\n",
    "    if s is None:\n",
    "        return ''\n",
    "    # Basic cleanup: normalize whitespace, remove odd control chars\n",
    "    s = s.replace(chr(9), ' ').replace(chr(13), ' ').replace(chr(10), ' ')\n",
    "    s = ' '.join(s.split())\n",
    "    return s\n",
    "\n",
    "def format_prompt_completion(example):\n",
    "    # Adapt field names to the dataset schema. Common fields: 'question' and 'answer' or similar.\n",
    "    # We'll try to handle a few variants robustly.\n",
    "    q = example.get('question') or example.get('problem') or example.get('prompt') or ''\n",
    "    a = example.get('answer') or example.get('solution') or example.get('target') or ''\n",
    "    q = clean_text(q)\n",
    "    a = clean_text(a)\n",
    "    # Compose the prompt and completion; ensure completion contains an end token or newline.\n",
    "    prompt = f'Question: {q}\\nAnswer:'\n",
    "    completion = ' ' + a + ' '  # leading space helps some tokenizers' alignment\n",
    "    return {'prompt': prompt, 'completion': completion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b309607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4c. Create train/validation split and save JSONL files\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path('./data')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def prepare_and_save(dset, split_name='train', val_frac=0.05, seed=42, max_items=None):\n",
    "    # flatten list of formatted items\n",
    "    items = []\n",
    "    for i, ex in enumerate(dset):\n",
    "        if max_items and i >= max_items:\n",
    "            break\n",
    "        formatted = format_prompt_completion(ex)\n",
    "        if formatted['prompt'].strip() and formatted['completion'].strip():\n",
    "            items.append(formatted)\n",
    "    print(f'Prepared {len(items)} cleaned examples from {split_name}')\n",
    "    random.Random(seed).shuffle(items)\n",
    "    cut = int(len(items) * (1 - val_frac))\n",
    "    train_items = items[:cut]\n",
    "    val_items = items[cut:]\n",
    "    # Save as JSONL\n",
    "    train_path = out_dir / f'{split_name}_train.jsonl'\n",
    "    val_path = out_dir / f'{split_name}_val.jsonl'\n",
    "    with open(train_path, 'w', encoding='utf-8') as f1, open(val_path, 'w', encoding='utf-8') as f2:\n",
    "        for it in train_items:\n",
    "            f1.write(json.dumps(it, ensure_ascii=False) + '\\n')\n",
    "        for it in val_items:\n",
    "            f2.write(json.dumps(it, ensure_ascii=False) + '\\n')\n",
    "    print('Saved', train_path, 'and', val_path)\n",
    "    return train_path, val_path\n",
    "\n",
    "# Run preparation if dataset loaded\n",
    "if ds is not None:\n",
    "    # Use first available split (often 'train') and limit items for quick tests\n",
    "    first_split = list(ds.keys())[0]\n",
    "    train_file, val_file = prepare_and_save(ds[first_split], split_name=first_split, val_frac=0.05, max_items=5000)\n",
    "else:\n",
    "    print('Dataset not loaded; please load dataset manually or provide local files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d3f8e",
   "metadata": {},
   "source": [
    "### 5B. Hugging Face Transformers + PEFT (LoRA) — runnable training pipeline\n",
    "This is a concrete training implementation that uses PEFT LoRA; it's widely supported and works well for parameter-efficient fine-tuning. It also demonstrates hyperparameter setup, checkpointing, and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9df015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "print('='*70)\n",
    "print('Hugging Face Authentication')\n",
    "print('='*70)\n",
    "print()\n",
    "print('Paste your HF token when prompted.')\n",
    "print('If you don\\'t have a token:')\n",
    "print('  1. Go to https://huggingface.co/settings/tokens')\n",
    "print('  2. Create a new token with \"Read\" permissions')\n",
    "print('  3. Paste it below')\n",
    "print()\n",
    "\n",
    "try:\n",
    "    login()\n",
    "    print('✓ Successfully authenticated with Hugging Face')\n",
    "except Exception as e:\n",
    "    print(f'❌ Authentication failed: {e}')\n",
    "    print('Please check your token and try again.')\n",
    "    raise\n",
    "\n",
    "print('='*70)\n",
    "print('Hugging Face Authentication')\n",
    "print('='*70)\n",
    "print()\n",
    "print('Paste your HF token when prompted.')\n",
    "print('If you don\\'t have a token:')\n",
    "print('  1. Go to https://huggingface.co/settings/tokens')\n",
    "print('  2. Create a new token with \"Read\" permissions')\n",
    "print('  3. Paste it below')\n",
    "print()\n",
    "\n",
    "try:\n",
    "    login()\n",
    "    print('✓ Successfully authenticated with Hugging Face')\n",
    "except Exception as e:\n",
    "    print(f'❌ Authentication failed: {e}')\n",
    "    print('Please check your token and try again.')\n",
    "    raise\n",
    "\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('Installing dependencies in Colab (this may take a minute)...')\n",
    "    get_ipython().system('pip install -q peft accelerate bitsandbytes evaluate transformers datasets')\n",
    "    print('Installation complete.')\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "# Try importing evaluate; if it fails, try installing it again\n",
    "try:\n",
    "    import evaluate\n",
    "    print('✓ evaluate module imported successfully')\n",
    "except ImportError:\n",
    "    print('Installing evaluate...')\n",
    "    if IN_COLAB:\n",
    "        get_ipython().system('pip install -q evaluate')\n",
    "        import evaluate\n",
    "        print('✓ evaluate installed and imported')\n",
    "    else:\n",
    "        print('⚠ evaluate not available. Install with: pip install evaluate')\n",
    "        evaluate = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d387dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4c315462ca4b55ad2e1a638aa9001b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()  # Paste HF token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799bf57",
   "metadata": {},
   "source": [
    "## 5. Hugging Face Authentication — Required for Llama 3.2 1B\n",
    "\n",
    "**Important:** Llama 3.2 1B is a **gated model** and requires authentication with a Hugging Face token.\n",
    "\n",
    "**Setup steps:**\n",
    "\n",
    "1. **Accept the model license** on Hugging Face:\n",
    "   - Go to [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B)\n",
    "   - Click \"Access repository\" and accept the license\n",
    "\n",
    "2. **Create a Hugging Face token:**\n",
    "   - Go to [Hugging Face Settings → Access Tokens](https://huggingface.co/settings/tokens)\n",
    "   - Click \"New token\", choose \"Read\" permissions\n",
    "   - Copy the token\n",
    "\n",
    "3. **Authenticate in this notebook:**\n",
    "   - Run the cell below and paste your token when prompted\n",
    "   - The token will be securely stored locally and used for all HF model loading\n",
    "\n",
    "**After authentication**, you can proceed with tokenization and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcbffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and dataset creation\n",
    "# Updated for Llama 3.2 1B model\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "model_name_or_path = 'meta-llama/Llama-3.2-1B'\n",
    "\n",
    "# Ensure model_name_or_path is set before running this cell\n",
    "if '<LLAMA' in model_name_or_path or not model_name_or_path or model_name_or_path.startswith('<'):\n",
    "    print('⚠ STOP: Set model_name_or_path first!')\n",
    "    print('  Example: model_name_or_path = \"meta-llama/Llama-2-7b-hf\"')\n",
    "    print('  Or for Llama 3.2 1B: model_name_or_path = \"meta-llama/Llama-3.2-1B\"')\n",
    "else:\n",
    "    print(f'Using model: {model_name_or_path}')\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f'Tokenizer loaded. Vocab size: {len(tokenizer)}')\n",
    "    \n",
    "    # Load JSONL datasets\n",
    "    train_jsonl = Path('./data/train_train.jsonl')\n",
    "    val_jsonl = Path('./data/train_val.jsonl')\n",
    "    \n",
    "    if not train_jsonl.exists() or not val_jsonl.exists():\n",
    "        print(f'⚠ JSONL files not found. Expected:')\n",
    "        print(f'  - {train_jsonl}')\n",
    "        print(f'  - {val_jsonl}')\n",
    "        print('Run the data preparation cells first.')\n",
    "    else:\n",
    "        def load_jsonl_to_dataset(path):\n",
    "            import json\n",
    "            items = []\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        items.append(json.loads(line))\n",
    "            return Dataset.from_list(items)\n",
    "        \n",
    "        print('Loading JSONL datasets...')\n",
    "        train_ds = load_jsonl_to_dataset(train_jsonl)\n",
    "        val_ds = load_jsonl_to_dataset(val_jsonl)\n",
    "        print(f'Train: {len(train_ds)} examples, Val: {len(val_ds)} examples')\n",
    "        \n",
    "        def tokenize_fn(batch):\n",
    "            # Concatenate prompt + completion for causal LM training\n",
    "            texts = [x['prompt'] + x['completion'] for x in batch]\n",
    "            out = tokenizer(texts, truncation=True, max_length=max_length, padding='max_length', return_tensors=None)\n",
    "            out['labels'] = out['input_ids'].copy()  # For causal LM, labels = input_ids\n",
    "            return out\n",
    "        \n",
    "        print('Tokenizing datasets (this may take a few minutes)...')\n",
    "        tokenized_train = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names, batch_size=32, num_proc=4)\n",
    "        tokenized_val = val_ds.map(tokenize_fn, batched=True, remove_columns=val_ds.column_names, batch_size=32, num_proc=4)\n",
    "        \n",
    "        print(f'Tokenized train: {len(tokenized_train)} samples')\n",
    "        print(f'Tokenized val: {len(tokenized_val)} samples')\n",
    "        \n",
    "        # Data collator for causal LM (pads to same length within batch, no MLM)\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "        \n",
    "        print('✓ Tokenization complete. Ready for training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b439762",
   "metadata": {},
   "source": [
    "### 5C. Tokenization setup — Llama 3.2 1B\n",
    "\n",
    "Before running this cell, set `model_name_or_path` to your Llama 3.2 1B model path.\n",
    "\n",
    "**To use Llama 3.2 1B:**\n",
    "1. Accept the model license on Hugging Face: [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B)\n",
    "2. In the previous cell, change:\n",
    "   ```python\n",
    "   model_name_or_path = 'meta-llama/Llama-3.2-1B'\n",
    "   ```\n",
    "3. If using Colab or restricted access, authenticate with Hugging Face:\n",
    "   ```python\n",
    "   from huggingface_hub import login\n",
    "   login()  # paste your token when prompted\n",
    "   ```\n",
    "4. Then run this cell to tokenize the dataset.\n",
    "\n",
    "The cell will:\n",
    "- Load the tokenizer and set up pad tokens\n",
    "- Load the prepared JSONL train/val data\n",
    "- Tokenize everything to `max_length=512`\n",
    "- Create a data collator ready for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5170a",
   "metadata": {},
   "source": [
    "## 5D. Model Loading — Llama 3.2 1B\n",
    "\n",
    "Load the base Llama 3.2 1B model with 4-bit quantization for memory efficiency. This cell requires:\n",
    "- ✓ HF token authentication (from cell above)\n",
    "- ✓ Tokenizer loaded (from tokenization cell)\n",
    "- ✓ Llama 3.2 1B license accepted on Hugging Face\n",
    "\n",
    "The model will be loaded with **4-bit quantization** to fit in GPU VRAM (tested on Google Colab T4/A100).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d67a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "print('='*70)\n",
    "print('Loading Llama 3.2 1B Model')\n",
    "print('='*70)\n",
    "\n",
    "# Configure 4-bit quantization for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(f'\\nLoading model: meta-llama/Llama-3.2-1B')\n",
    "    print('(This may take 1-2 minutes on first load...)\\n')\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        'meta-llama/Llama-3.2-1B',\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"flash_attention_2\",  # Use Flash Attention if available\n",
    "    )\n",
    "    \n",
    "    print('✓ Model loaded successfully!')\n",
    "    print(f'Model dtype: {model.dtype}')\n",
    "    print(f'Model device: {next(model.parameters()).device}')\n",
    "    print(f'\\nModel is ready for training with LoRA adapters.')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'❌ Failed to load model: {e}')\n",
    "    print()\n",
    "    print('Troubleshooting:')\n",
    "    print('1. Ensure you accepted the license at: https://huggingface.co/meta-llama/Llama-3.2-1B')\n",
    "    print('2. Verify your HF token has read access: https://huggingface.co/settings/tokens')\n",
    "    print('3. If you\\'re not authenticated, run the login cell above')\n",
    "    print()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c8b99",
   "metadata": {},
   "source": [
    "#### Training arguments, early stopping and checkpointing\n",
    "We'll configure Trainer/TrainingArguments and add an EarlyStoppingCallback to stop when validation loss plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907866ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example TrainingArguments and EarlyStoppingCallback usage (uncomment to run)\n",
    "# from transformers import EarlyStoppingCallback\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     per_device_train_batch_size=per_device_train_batch_size,\n",
    "#     per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "#     evaluation_strategy=evaluation_strategy,\n",
    "#     save_strategy=save_strategy,\n",
    "#     num_train_epochs=num_train_epochs,\n",
    "#     learning_rate=learning_rate,\n",
    "#     logging_steps=logging_steps,\n",
    "#     fp16=fp16,\n",
    "#     gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#     save_total_limit=3,\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "#\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_train,\n",
    "#     eval_dataset=tokenized_val,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "#\n",
    "# trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n",
    "# trainer.train()\n",
    "print('Training arguments and trainer skeleton provided. Run when datasets and model are loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984c4f9",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Analysis\n",
    "We implement: (a) a simple exact-match style metric (normalized whitespace and case-insensitive), (b) generation examples before/after fine-tuning, and (c) a short analysis template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c51f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab setup: mount Drive (optional) and install dependencies\n",
    "# Run this cell in Google Colab (it will skip installs when not in Colab)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    print('Mounting Google Drive...')\n",
    "    drive.mount('/content/drive')\n",
    "    print('Upgrading pip and installing dependencies (this may take a few minutes)')\n",
    "    # Core dependencies used by this notebook; adjust as needed\n",
    "    !pip install -q --upgrade pip\n",
    "    !pip install -q git+https://github.com/unslothai/unsloth.git\n",
    "    !pip install -q transformers datasets accelerate peft bitsandbytes evaluate sentencepiece safetensors\n",
    "    # Optional: install huggingface hub to access gated weights if needed\n",
    "    !pip install -q huggingface_hub\n",
    "    import torch\n",
    "    print('Install finished. PyTorch:', torch.__version__, 'CUDA available:', torch.cuda.is_available())\n",
    "else:\n",
    "    print('Not running in Colab. To use GPU, open this notebook in Google Colab (Runtime -> Change runtime type -> GPU).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0476d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
