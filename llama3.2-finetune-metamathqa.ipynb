{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405f004f",
   "metadata": {},
   "source": [
    "# Open this notebook in Google Colab\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/karukan/llamaFinetuning/blob/main/llama3.2-finetune-metamathqa.ipynb)\n",
    "\n",
    "Click the badge to open this notebook in Colab. If the link doesn't match your GitHub location, replace `karukan/llamaFinetuning` and `main` in the URL with your `user/repo` and branch name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62eef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "print('Python:', sys.version.splitlines()[0])\n",
    "print('PyTorch:', torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA GPU detected:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print('No CUDA GPU detected. To run this notebook with a GPU open it in Colab and select Runtime -> Change runtime type -> GPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe Colab mount & install (runs only when executed inside Google Colab)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Install required packages (adjust as needed)\n",
    "    !pip install --upgrade pip\n",
    "    !pip install git+https://github.com/unslothai/unsloth.git\n",
    "    import torch\n",
    "    print('Installed unsloth. GPU available:', torch.cuda.is_available())\n",
    "else:\n",
    "    print('Not running in Colab. To use GPU, open this notebook in Google Colab (Runtime -> Change runtime type -> GPU).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb388f",
   "metadata": {},
   "source": [
    "# Fine-tune Llama 3.2 1B on meta-math/MetaMathQA (using `unsloth` + HF/PEFT fallback)\n",
    "\n",
    "This notebook documents environment setup, data preparation, fine-tuning, evaluation, and a short analytical report about fine-tuning techniques. It includes:\n",
    "\n",
    "- GPU & dependency setup (PowerShell-friendly commands)\n",
    "- Loading and cleaning the `meta-math/MetaMathQA` dataset\n",
    "- Formatting data into prompt/completion JSONL suitable for unsloth or Hugging Face training\n",
    "- Example `unsloth` training command (placeholder) and a concrete Hugging Face + PEFT (LoRA) training implementation as a robust alternative\n",
    "- Evaluation: pre/post generation examples and a simple exact-match metric\n",
    "- Notes on checkpoints, early stopping, challenges and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc8400",
   "metadata": {},
   "source": [
    "## Assumptions & prerequisites\n",
    "\n",
    "1. You have GPU(s) available with CUDA drivers compatible with your PyTorch/CUDA setup.\n",
    "2. You have access to Llama 3.2 1B weights (Hugging Face repo or local checkpoint). If the weights are gated, ensure you have the necessary access token and follow the license terms.\n",
    "3. `unsloth` is requested by the assignment. This notebook provides an `unsloth`-style CLI example and a fully working Hugging Face + PEFT alternative if `unsloth` isn't available or if you prefer more control.\n",
    "4. The notebook uses Python; run it in an environment with GPU-enabled PyTorch, and install packages as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c138415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Environment Setup (commands for PowerShell).\n",
    "# Run these in a terminal (PowerShell) or use the same lines in a notebook cell prefixed with '!' if you run inside Jupyter.\n",
    "# Notes: pick Python 3.10+ (3.11 is fine). Adjust versions if needed for your CUDA/PyTorch combo.\n",
    "\n",
    "print('--- Example PowerShell commands (do NOT run this print as commands).')\n",
    "print('Install base packages:')\n",
    "print('python -m pip install --upgrade pip')\n",
    "print('pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118')\n",
    "print('pip install transformers datasets accelerate evaluate peft bitsandbytes sentencepiece')\n",
    "print('pip install unsloth  # if available on PyPI; otherwise follow unsloth install docs')\n",
    "print('pip install git+https://github.com/huggingface/transformers.git  # optional dev')\n",
    "print('pip install accelerate')\n",
    "print('Optional (faster): pip install einops')\n",
    "\n",
    "print('\n",
    ",\n",
    "`')\n",
    "print('- If using bitsandbytes, ensure your CUDA and bitsandbytes versions are compatible')\n",
    "print('- If weights are on Hugging Face and gated, set HF_TOKEN env var: $env:HF_TOKEN = \"<your_token>\" (PowerShell)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd767b",
   "metadata": {},
   "source": [
    "### 3a. Quick verification (run in a Python cell to verify GPU access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('CUDA available?', torch.cuda.is_available())\n",
    "print('CUDA devices:', torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print('Current device name:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc993b1a",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "We load `meta-math/MetaMathQA` via the `datasets` library, clean the text, and format prompt-completion pairs. The target format used below is a JSONL where each line is {\"prompt\":..., \"completion\":...} suitable for many LLM fine-tuning tools.\n",
    "Option 2 justification: `MetaMathQA` is chosen because it contains math reasoning Q&A examples that can help the model specialize in formal mathematical problem phrasing and solution generation—useful for benchmarking math reasoning improvements after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json, os\n",
    "\n",
    "# Load dataset from the hub. If you have it locally, adapt the path.\n",
    "dataset_name = 'meta-math/MetaMathQA'\n",
    "print('Loading dataset:', dataset_name)\n",
    "try:\n",
    "    ds = load_dataset(dataset_name)\n",
    "except Exception as e:\n",
    "    print('Failed to load directly. Check network/access or replace with local path. Error:', e)\n",
    "    ds = None\n",
    "\n",
    "# Inspect if loaded\n",
    "if ds is not None:\n",
    "    print(ds)\n",
    "    # show a few examples (train split may be named 'train')\n",
    "    for k in ds.keys():\n",
    "        print('Split', k, '->', ds[k].num_rows)\n",
    "    print('\",\n",
    "Example row (first train if exists):')\n",
    "    split = list(ds.keys())[0]\n",
    "    print(ds[split][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc7289",
   "metadata": {},
   "source": [
    "### 4b. Data cleaning and formatting helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209afa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(s):\n",
    "    if s is None:\n",
    "        return ''\n",
    "    # Basic cleanup: normalize whitespace, remove odd control chars\n",
    "    s = s.replace('\t', ' ').replace('\",\n",
    "', '\",\n",
    "')\n",
    "    s = re.sub(r'+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "def format_prompt_completion(example):\n",
    "    # Adapt field names to the dataset schema. Common fields: 'question' and 'answer' or similar.\n",
    "    # We'll try to handle a few variants robustly.\n",
    "    q = example.get('question') or example.get('problem') or example.get('prompt') or ''\n",
    "    a = example.get('answer') or example.get('solution') or example.get('target') or ''\n",
    "    q = clean_text(q)\n",
    "    a = clean_text(a)\n",
    "    # Compose the prompt and completion; ensure completion contains an end token or newline.\n",
    "    prompt = f'Question: {q}\",\n",
    "Answer:'\n",
    "    completion = ' ' + a + ' '  # leading space helps some tokenizers' alignment\n",
    "    return {'prompt': prompt, 'completion': completion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b309607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4c. Create train/validation split and save JSONL files\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path('./data')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def prepare_and_save(dset, split_name='train', val_frac=0.05, seed=42, max_items=None):\n",
    "    # flatten list of formatted items\n",
    "    items = []\n",
    "    for i, ex in enumerate(dset):\n",
    "        if max_items and i >= max_items:\n",
    "            break\n",
    "        formatted = format_prompt_completion(ex)\n",
    "        if formatted['prompt'].strip() and formatted['completion'].strip():\n",
    "            items.append(formatted)\n",
    "    print(f'Prepared {len(items)} cleaned examples from {split_name}')\n",
    "    random.Random(seed).shuffle(items)\n",
    "    cut = int(len(items) * (1 - val_frac))\n",
    "    train_items = items[:cut]\n",
    "    val_items = items[cut:]\n",
    "    # Save as JSONL\n",
    "    train_path = out_dir / f'{split_name}_train.jsonl'\n",
    "    val_path = out_dir / f'{split_name}_val.jsonl'\n",
    "    with open(train_path, 'w', encoding='utf-8') as f1, open(val_path, 'w', encoding='utf-8') as f2:\n",
    "        for it in train_items:\n",
    "            f1.write(json.dumps(it, ensure_ascii=False) + '\",',\n",
    ",\n",
    ",',\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d3f8e",
   "metadata": {},
   "source": [
    "### 5B. Hugging Face Transformers + PEFT (LoRA) — runnable training pipeline\n",
    "This is a concrete training implementation that uses PEFT LoRA; it's widely supported and works well for parameter-efficient fine-tuning. It also demonstrates hyperparameter setup, checkpointing, and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9df015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any missing dependencies in notebook if desired (uncomment to run).\n",
    "# !pip install -q peft accelerate bitsandbytes evaluate transformers datasets\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "# Hyperparameters (5a)\n",
    "model_name_or_path = '<LLAMA_3_2_1B_HF_ID_OR_LOCAL_PATH>'  # replace with the real repo id or path\n",
    "output_dir = './hf_peft_output'\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 3\n",
    "save_strategy = 'epoch'\n",
    "evaluation_strategy = 'epoch'\n",
    "logging_strategy = 'steps'\n",
    "logging_steps = 100\n",
    "fp16 = True\n",
    "gradient_accumulation_steps = 1\n",
    "weight_decay = 0.01\n",
    "warmup_steps = 50\n",
    "max_length = 512\n",
    "\n",
    "# Prepare tokenizer and model (may use 8-bit/bitsandbytes to reduce memory)\n",
    "print('Loading tokenizer and model placeholder (do not run until you set model_name_or_path)')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='auto', load_in_8bit=True, torch_dtype=torch.float16)\n",
    "\n",
    "# PEFT/LoRA config (small ranks for 1B model)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q_proj','v_proj'] , # adapt depending on model architecture\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    "markdown\n",
    "#VSC-0480a324\n",
    "markdown\n",
    "# Open this notebook in Google Colab\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/karukan/llamaFinetuning/blob/main/llama3.2-finetune-metamathqa.ipynb)\n",
    "\n",
    "Click the badge to open this notebook in Colab. If the link doesn't match your GitHub location, replace `karukan/llamaFinetuning` and `main` in the URL with your `user/repo` and branch name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcbffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and dataset creation (uncomment & adapt when model set)\n",
    "# from pathlib import Path\n",
    "# train_jsonl = Path('./data/train.jsonl')\n",
    "# val_jsonl = Path('./data/val.jsonl')\n",
    "#\n",
    "# def load_jsonl_to_dataset(path):\n",
    "#     import json\n",
    "#     items = []\n",
    "#     with open(path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             items.append(json.loads(line))\n",
    "#     return Dataset.from_list(items)\n",
    "#\n",
    "# train_ds = load_jsonl_to_dataset(train_jsonl)\n",
    "# val_ds = load_jsonl_to_dataset(val_jsonl)\n",
    "#\n",
    "# def tokenize_fn(batch):\n",
    "#     # concat prompt and completion so model predicts completion tokens; optionally shift labels to only include completion tokens\n",
    "#     texts = [x['prompt'] + x['completion'] for x in batch]\n",
    "#     out = tokenizer(texts, truncation=True, max_length=max_length, padding='max_length')\n",
    "#     input_ids = out['input_ids']\n",
    "#     out['labels'] = [[-100]*len(i) for i in input_ids]  # simple placeholder: refine for causal LM\n",
    "#     # For simplicity here we set labels=input_ids so model learns to reconstruct; for instruction tuning mask prompt tokens if desired\n",
    "#     out['labels'] = input_ids\n",
    "#     return out\n",
    "#\n",
    "# tokenized_train = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names)\n",
    "# tokenized_val = val_ds.map(tokenize_fn, batched=True, remove_columns=val_ds.column_names)\n",
    "#\n",
    "# Data collator for causal LM (no MLM masking)\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "print('Tokenization cell prepared. Uncomment and run when your model and tokenizer are set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c8b99",
   "metadata": {},
   "source": [
    "#### Training arguments, early stopping and checkpointing\n",
    "We'll configure Trainer/TrainingArguments and add an EarlyStoppingCallback to stop when validation loss plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907866ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example TrainingArguments and EarlyStoppingCallback usage (uncomment to run)\n",
    "# from transformers import EarlyStoppingCallback\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     per_device_train_batch_size=per_device_train_batch_size,\n",
    "#     per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "#     evaluation_strategy=evaluation_strategy,\n",
    "#     save_strategy=save_strategy,\n",
    "#     num_train_epochs=num_train_epochs,\n",
    "#     learning_rate=learning_rate,\n",
    "#     logging_steps=logging_steps,\n",
    "#     fp16=fp16,\n",
    "#     gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#     save_total_limit=3,\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "#\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_train,\n",
    "#     eval_dataset=tokenized_val,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "#\n",
    "# trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n",
    "# trainer.train()\n",
    "print('Training arguments and trainer skeleton provided. Run when datasets and model are loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984c4f9",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Analysis\n",
    "We implement: (a) a simple exact-match style metric (normalized whitespace and case-insensitive), (b) generation examples before/after fine-tuning, and (c) a short analysis template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c51f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab setup: mount Drive (optional) and install dependencies\n",
    "# Run this cell in Google Colab (it will skip installs when not in Colab)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    print('Mounting Google Drive...')\n",
    "    drive.mount('/content/drive')\n",
    "    print('Upgrading pip and installing dependencies (this may take a few minutes)')\n",
    "    # Core dependencies used by this notebook; adjust as needed\n",
    "    !pip install -q --upgrade pip\n",
    "    !pip install -q git+https://github.com/unslothai/unsloth.git\n",
    "    !pip install -q transformers datasets accelerate peft bitsandbytes evaluate sentencepiece safetensors\n",
    "    # Optional: install huggingface hub to access gated weights if needed\n",
    "    !pip install -q huggingface_hub\n",
    "    import torch\n",
    "    print('Install finished. PyTorch:', torch.__version__, 'CUDA available:', torch.cuda.is_available())\n",
    "else:\n",
    "    print('Not running in Colab. To use GPU, open this notebook in Google Colab (Runtime -> Change runtime type -> GPU).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0476d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
