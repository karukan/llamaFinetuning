{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84569354",
   "metadata": {},
   "source": [
    "# Llama 3.2 — MetaMathQA fine-tuning (Colab-ready)\n",
    "This notebook prepares MetaMathQA data and demonstrates a PEFT/LoRA fine-tuning workflow suitable for Llama-family causal models. Run in Google Colab with a GPU runtime for best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c95182",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "This notebook contains: (1) environment and Colab quickstart, (2) data preparation for MetaMathQA, (3) example training using Hugging Face Transformers + PEFT (LoRA), and (4) evaluation examples.\n",
    "\n",
    "Intended usage: open in Colab (Runtime → Change runtime type → GPU), run the setup cell, prepare data, then run the training cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24fe75",
   "metadata": {},
   "source": [
    "## 2. Environment & Colab quickstart\n",
    "If you run this notebook locally without a CUDA GPU, training will fail or be extremely slow — prefer Colab or other GPU hosts.\n",
    "\n",
    "Open in Colab: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/karukan/llamaFinetuning/blob/main/llama3.2-finetune-metamathqa.ipynb)\n",
    "\n",
    "Quick steps: set Runtime→Change runtime type→GPU, run the setup cell (mount Drive if you want checkpoints persisted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('CUDA available?', torch.cuda.is_available())\n",
    "print('CUDA devices:', torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print('Current device name:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc993b1a",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "We load `meta-math/MetaMathQA` via the `datasets` library, clean the text, and format prompt-completion pairs. The target format used below is a JSONL where each line is {\"prompt\":..., \"completion\":...} suitable for many LLM fine-tuning tools.\n",
    "Option 2 justification: `MetaMathQA` is chosen because it contains math reasoning Q&A examples that can help the model specialize in formal mathematical problem phrasing and solution generation—useful for benchmarking math reasoning improvements after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json, os\n",
    "\n",
    "# Load dataset from the hub. If you have it locally, adapt the path.\n",
    "dataset_name = 'meta-math/MetaMathQA'\n",
    "print('Loading dataset:', dataset_name)\n",
    "try:\n",
    "    ds = load_dataset(dataset_name)\n",
    "except Exception as e:\n",
    "    print('Failed to load directly. Check network/access or replace with local path. Error:', e)\n",
    "    ds = None\n",
    "\n",
    "# Inspect if loaded\n",
    "if ds is not None:\n",
    "    print(ds)\n",
    "    # show a few examples (train split may be named 'train')\n",
    "    for k in ds.keys():\n",
    "        print('Split', k, '->', ds[k].num_rows)\n",
    "    print('Example row (first train if exists):')\n",
    "    split = list(ds.keys())[0]\n",
    "    print(ds[split][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc7289",
   "metadata": {},
   "source": [
    "### 4b. Data cleaning and formatting helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209afa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(s):\n",
    "    if s is None:\n",
    "        return ''\n",
    "    # Basic cleanup: normalize whitespace, remove odd control chars\n",
    "    s = s.replace(chr(9), ' ').replace(chr(13), ' ').replace(chr(10), ' ')\n",
    "    s = ' '.join(s.split())\n",
    "    return s\n",
    "\n",
    "def format_prompt_completion(example):\n",
    "    # Adapt field names to the dataset schema. Common fields: 'question' and 'answer' or similar.\n",
    "    # We'll try to handle a few variants robustly.\n",
    "    q = example.get('question') or example.get('problem') or example.get('prompt') or ''\n",
    "    a = example.get('answer') or example.get('solution') or example.get('target') or ''\n",
    "    q = clean_text(q)\n",
    "    a = clean_text(a)\n",
    "    # Compose the prompt and completion; ensure completion contains an end token or newline.\n",
    "    prompt = f'Question: {q}\\nAnswer:'\n",
    "    completion = ' ' + a + ' '  # leading space helps some tokenizers' alignment\n",
    "    return {'prompt': prompt, 'completion': completion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b309607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4c. Create train/validation split and save JSONL files\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path('./data')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def prepare_and_save(dset, split_name='train', val_frac=0.05, seed=42, max_items=None):\n",
    "    # flatten list of formatted items\n",
    "    items = []\n",
    "    for i, ex in enumerate(dset):\n",
    "        if max_items and i >= max_items:\n",
    "            break\n",
    "        formatted = format_prompt_completion(ex)\n",
    "        if formatted['prompt'].strip() and formatted['completion'].strip():\n",
    "            items.append(formatted)\n",
    "    print(f'Prepared {len(items)} cleaned examples from {split_name}')\n",
    "    random.Random(seed).shuffle(items)\n",
    "    cut = int(len(items) * (1 - val_frac))\n",
    "    train_items = items[:cut]\n",
    "    val_items = items[cut:]\n",
    "    # Save as JSONL\n",
    "    train_path = out_dir / f'{split_name}_train.jsonl'\n",
    "    val_path = out_dir / f'{split_name}_val.jsonl'\n",
    "    with open(train_path, 'w', encoding='utf-8') as f1, open(val_path, 'w', encoding='utf-8') as f2:\n",
    "        for it in train_items:\n",
    "            f1.write(json.dumps(it, ensure_ascii=False) + '\\n')\n",
    "        for it in val_items:\n",
    "            f2.write(json.dumps(it, ensure_ascii=False) + '\\n')\n",
    "    print('Saved', train_path, 'and', val_path)\n",
    "    return train_path, val_path\n",
    "\n",
    "# Run preparation if dataset loaded\n",
    "if ds is not None:\n",
    "    # Use first available split (often 'train') and limit items for quick tests\n",
    "    first_split = list(ds.keys())[0]\n",
    "    train_file, val_file = prepare_and_save(ds[first_split], split_name=first_split, val_frac=0.05, max_items=5000)\n",
    "else:\n",
    "    print('Dataset not loaded; please load dataset manually or provide local files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d3f8e",
   "metadata": {},
   "source": [
    "### 5B. Hugging Face Transformers + PEFT (LoRA) — runnable training pipeline\n",
    "This is a concrete training implementation that uses PEFT LoRA; it's widely supported and works well for parameter-efficient fine-tuning. It also demonstrates hyperparameter setup, checkpointing, and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9df015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any missing dependencies in notebook if desired (uncomment to run).\n",
    "# !pip install -q peft accelerate bitsandbytes evaluate transformers datasets\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "# Hyperparameters (5a)\n",
    "model_name_or_path = '<LLAMA_3_2_1B_HF_ID_OR_LOCAL_PATH>'  # replace with the real repo id or path\n",
    "output_dir = './hf_peft_output'\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 3\n",
    "save_strategy = 'epoch'\n",
    "evaluation_strategy = 'epoch'\n",
    "logging_strategy = 'steps'\n",
    "logging_steps = 100\n",
    "fp16 = True\n",
    "gradient_accumulation_steps = 1\n",
    "weight_decay = 0.01\n",
    "warmup_steps = 50\n",
    "max_length = 512\n",
    "\n",
    "# Prepare tokenizer and model (may use 8-bit/bitsandbytes to reduce memory)\n",
    "print('Loading tokenizer and model placeholder (do not run until you set model_name_or_path)')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='auto', load_in_8bit=True, torch_dtype=torch.float16)\n",
    "\n",
    "# PEFT/LoRA config (small ranks for 1B model)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q_proj','v_proj'],  # adapt depending on model architecture\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "print('PEFT/LoRA config defined (placeholder)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcbffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and dataset creation (uncomment & adapt when model set)\n",
    "# from pathlib import Path\n",
    "# train_jsonl = Path('./data/train.jsonl')\n",
    "# val_jsonl = Path('./data/val.jsonl')\n",
    "#\n",
    "# def load_jsonl_to_dataset(path):\n",
    "#     import json\n",
    "#     items = []\n",
    "#     with open(path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             items.append(json.loads(line))\n",
    "#     return Dataset.from_list(items)\n",
    "#\n",
    "# train_ds = load_jsonl_to_dataset(train_jsonl)\n",
    "# val_ds = load_jsonl_to_dataset(val_jsonl)\n",
    "#\n",
    "# def tokenize_fn(batch):\n",
    "#     # concat prompt and completion so model predicts completion tokens; optionally shift labels to only include completion tokens\n",
    "#     texts = [x['prompt'] + x['completion'] for x in batch]\n",
    "#     out = tokenizer(texts, truncation=True, max_length=max_length, padding='max_length')\n",
    "#     input_ids = out['input_ids']\n",
    "#     out['labels'] = [[-100]*len(i) for i in input_ids]  # simple placeholder: refine for causal LM\n",
    "#     # For simplicity here we set labels=input_ids so model learns to reconstruct; for instruction tuning mask prompt tokens if desired\n",
    "#     out['labels'] = input_ids\n",
    "#     return out\n",
    "#\n",
    "# tokenized_train = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names)\n",
    "# tokenized_val = val_ds.map(tokenize_fn, batched=True, remove_columns=val_ds.column_names)\n",
    "#\n",
    "# Data collator for causal LM (no MLM masking)\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "print('Tokenization cell prepared. Uncomment and run when your model and tokenizer are set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c8b99",
   "metadata": {},
   "source": [
    "#### Training arguments, early stopping and checkpointing\n",
    "We'll configure Trainer/TrainingArguments and add an EarlyStoppingCallback to stop when validation loss plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907866ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example TrainingArguments and EarlyStoppingCallback usage (uncomment to run)\n",
    "# from transformers import EarlyStoppingCallback\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     per_device_train_batch_size=per_device_train_batch_size,\n",
    "#     per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "#     evaluation_strategy=evaluation_strategy,\n",
    "#     save_strategy=save_strategy,\n",
    "#     num_train_epochs=num_train_epochs,\n",
    "#     learning_rate=learning_rate,\n",
    "#     logging_steps=logging_steps,\n",
    "#     fp16=fp16,\n",
    "#     gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#     save_total_limit=3,\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "#\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_train,\n",
    "#     eval_dataset=tokenized_val,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "#\n",
    "# trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n",
    "# trainer.train()\n",
    "print('Training arguments and trainer skeleton provided. Run when datasets and model are loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984c4f9",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Analysis\n",
    "We implement: (a) a simple exact-match style metric (normalized whitespace and case-insensitive), (b) generation examples before/after fine-tuning, and (c) a short analysis template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c51f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab setup: mount Drive (optional) and install dependencies\n",
    "# Run this cell in Google Colab (it will skip installs when not in Colab)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    print('Mounting Google Drive...')\n",
    "    drive.mount('/content/drive')\n",
    "    print('Upgrading pip and installing dependencies (this may take a few minutes)')\n",
    "    # Core dependencies used by this notebook; adjust as needed\n",
    "    !pip install -q --upgrade pip\n",
    "    !pip install -q git+https://github.com/unslothai/unsloth.git\n",
    "    !pip install -q transformers datasets accelerate peft bitsandbytes evaluate sentencepiece safetensors\n",
    "    # Optional: install huggingface hub to access gated weights if needed\n",
    "    !pip install -q huggingface_hub\n",
    "    import torch\n",
    "    print('Install finished. PyTorch:', torch.__version__, 'CUDA available:', torch.cuda.is_available())\n",
    "else:\n",
    "    print('Not running in Colab. To use GPU, open this notebook in Google Colab (Runtime -> Change runtime type -> GPU).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0476d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
