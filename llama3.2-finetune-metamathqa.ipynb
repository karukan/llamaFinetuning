{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84569354",
   "metadata": {},
   "source": [
    "# V1.2 Llama 3.1  — MetaMathQA fine-tuning (Colab-ready)\n",
    "This notebook prepares MetaMathQA data and demonstrates a PEFT/LoRA fine-tuning workflow suitable for Llama-family causal models. Run in Google Colab with a GPU runtime for best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c95182",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "This notebook contains: (1) environment and Colab quickstart, (2) data preparation for MetaMathQA, (3) example training using Hugging Face Transformers + PEFT (LoRA), and (4) evaluation examples.\n",
    "\n",
    "Intended usage: open in Colab (Runtime → Change runtime type → GPU), run the setup cell, prepare data, then run the training cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24fe75",
   "metadata": {},
   "source": [
    "## 2. Environment & Colab quickstart\n",
    "If you run this notebook locally without a CUDA GPU, training will fail or be extremely slow — prefer Colab or other GPU hosts.\n",
    "\n",
    "Open in Colab: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/karukan/llamaFinetuning/blob/main/llama3.2-finetune-metamathqa.ipynb)\n",
    "\n",
    "Quick steps: set Runtime→Change runtime type→GPU, run the setup cell (mount Drive if you want checkpoints persisted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('CUDA available?', torch.cuda.is_available())\n",
    "print('CUDA devices:', torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print('Current device name:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc993b1a",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "We load `meta-math/MetaMathQA` via the `datasets` library, clean the text, and format prompt-completion pairs. The target format used below is a JSONL where each line is {\"prompt\":..., \"completion\":...} suitable for many LLM fine-tuning tools.\n",
    "Option 2 justification: `MetaMathQA` is chosen because it contains math reasoning Q&A examples that can help the model specialize in formal mathematical problem phrasing and solution generation—useful for benchmarking math reasoning improvements after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json, os\n",
    "\n",
    "# Load dataset from the hub. If you have it locally, adapt the path.\n",
    "dataset_name = 'meta-math/MetaMathQA'\n",
    "print('Loading dataset:', dataset_name)\n",
    "try:\n",
    "    ds = load_dataset(dataset_name)\n",
    "except Exception as e:\n",
    "    print('Failed to load directly. Check network/access or replace with local path. Error:', e)\n",
    "    ds = None\n",
    "\n",
    "# Inspect if loaded\n",
    "if ds is not None:\n",
    "    print(ds)\n",
    "    # show a few examples (train split may be named 'train')\n",
    "    for k in ds.keys():\n",
    "        print('Split', k, '->', ds[k].num_rows)\n",
    "    print('Example row (first train if exists):')\n",
    "    split = list(ds.keys())[0]\n",
    "    print(ds[split][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc7289",
   "metadata": {},
   "source": [
    "### 4b. Data cleaning and formatting helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209afa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(s):\n",
    "    if s is None:\n",
    "        return ''\n",
    "    # Basic cleanup: normalize whitespace, remove odd control chars\n",
    "    s = s.replace(chr(9), ' ').replace(chr(13), ' ').replace(chr(10), ' ')\n",
    "    s = ' '.join(s.split())\n",
    "    return s\n",
    "\n",
    "def format_prompt_completion(example):\n",
    "    # Adapt field names to the dataset schema. Common fields: 'question' and 'answer' or similar.\n",
    "    # We'll try to handle a few variants robustly.\n",
    "    q = example.get('question') or example.get('problem') or example.get('prompt') or ''\n",
    "    a = example.get('answer') or example.get('solution') or example.get('target') or ''\n",
    "    q = clean_text(q)\n",
    "    a = clean_text(a)\n",
    "    # Compose the prompt and completion; ensure completion contains an end token or newline.\n",
    "    prompt = f'Question: {q}\\nAnswer:'\n",
    "    completion = ' ' + a + ' '  # leading space helps some tokenizers' alignment\n",
    "    return {'prompt': prompt, 'completion': completion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b309607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4c. Create train/validation split and save JSONL files\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path('./data')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def prepare_and_save(dset, split_name='train', val_frac=0.05, seed=42, max_items=None):\n",
    "    # flatten list of formatted items\n",
    "    items = []\n",
    "    for i, ex in enumerate(dset):\n",
    "        if max_items and i >= max_items:\n",
    "            break\n",
    "        formatted = format_prompt_completion(ex)\n",
    "        if formatted['prompt'].strip() and formatted['completion'].strip():\n",
    "            items.append(formatted)\n",
    "    print(f'Prepared {len(items)} cleaned examples from {split_name}')\n",
    "    random.Random(seed).shuffle(items)\n",
    "    cut = int(len(items) * (1 - val_frac))\n",
    "    train_items = items[:cut]\n",
    "    val_items = items[cut:]\n",
    "    # Save as JSONL\n",
    "    train_path = out_dir / f'{split_name}_train.jsonl'\n",
    "    val_path = out_dir / f'{split_name}_val.jsonl'\n",
    "    with open(train_path, 'w', encoding='utf-8') as f1, open(val_path, 'w', encoding='utf-8') as f2:\n",
    "        for it in train_items:\n",
    "            f1.write(json.dumps(it, ensure_ascii=False) + '\\n')\n",
    "        for it in val_items:\n",
    "            f2.write(json.dumps(it, ensure_ascii=False) + '\\n')\n",
    "    print('Saved', train_path, 'and', val_path)\n",
    "    return train_path, val_path\n",
    "\n",
    "# Run preparation if dataset loaded\n",
    "if ds is not None:\n",
    "    # Use first available split (often 'train') and limit items for quick tests\n",
    "    first_split = list(ds.keys())[0]\n",
    "    train_file, val_file = prepare_and_save(ds[first_split], split_name=first_split, val_frac=0.05, max_items=5000)\n",
    "else:\n",
    "    print('Dataset not loaded; please load dataset manually or provide local files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d3f8e",
   "metadata": {},
   "source": [
    "### 5B. Hugging Face Transformers + PEFT (LoRA) — runnable training pipeline\n",
    "This is a concrete training implementation that uses PEFT LoRA; it's widely supported and works well for parameter-efficient fine-tuning. It also demonstrates hyperparameter setup, checkpointing, and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9df015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies for Colab environment\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('Installing dependencies in Colab (this may take a minute)...')\n",
    "    get_ipython().system('pip install -q unsloth peft accelerate bitsandbytes evaluate transformers datasets')\n",
    "    print('Installation complete.')\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "# Try importing evaluate; if it fails, try installing it again\n",
    "try:\n",
    "    import evaluate\n",
    "    print('✓ evaluate module imported successfully')\n",
    "except ImportError:\n",
    "    print('Installing evaluate...')\n",
    "    if IN_COLAB:\n",
    "        get_ipython().system('pip install -q evaluate')\n",
    "        import evaluate\n",
    "        print('✓ evaluate installed and imported')\n",
    "    else:\n",
    "        print('⚠ evaluate not available. Install with: pip install evaluate')\n",
    "        evaluate = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d387dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4c315462ca4b55ad2e1a638aa9001b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optional: HuggingFace login (only needed for private/gated models)\n",
    "# For Unsloth pre-quantized Llama 3.1 8B, this is NOT required\n",
    "# Uncomment below if you need to access private models\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login()  # Paste HF token if needed\n",
    "\n",
    "print('✓ Skipping HF login (Unsloth model is open-access)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799bf57",
   "metadata": {},
   "source": [
    "## 5. Model & Environment Setup (Option B: Unsloth Pre-quantized)\n",
    "\n",
    "**Option B: Unsloth's Pre-quantized Llama 3.1 8B** (Recommended for Colab)\n",
    "\n",
    "✅ **No authentication required** — model is already quantized and available on HuggingFace  \n",
    "✅ **Faster inference** — 2x speedup with Unsloth  \n",
    "✅ **Lower memory** — 70% less VRAM than standard quantization  \n",
    "\n",
    "**Setup:**\n",
    "- Unsloth will automatically download the pre-quantized Llama 3.1 8B model\n",
    "- No token required — fully open-access model\n",
    "- Proceed directly to tokenization and training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcbffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and dataset creation\n",
    "# Updated for Llama 3.1 8B (Unsloth pre-quantized)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "model_name_or_path = 'unsloth/Llama-3.1-8B-4bit'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b439762",
   "metadata": {},
   "source": [
    "### 5C. Tokenization Setup — Llama 3.1 8B (Unsloth Pre-quantized)\n",
    "\n",
    "Using **Unsloth's pre-quantized Llama 3.1 8B**:\n",
    "- Model: `unsloth/Llama-3.1-8B-4bit`\n",
    "- No authentication required\n",
    "- Already 4-bit quantized for GPU memory efficiency\n",
    "\n",
    "**Before running this cell:**\n",
    "1. Ensure dependencies are installed (run setup cell above)\n",
    "2. The tokenizer and datasets will be loaded automatically\n",
    "3. Expected to tokenize ~5000 examples in 2-3 minutes\n",
    "\n",
    "The cell will:\n",
    "- Load the tokenizer for Llama 3.1 8B\n",
    "- Load the prepared JSONL train/val data\n",
    "- Tokenize everything to `max_length=512`\n",
    "- Create a data collator ready for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5170a",
   "metadata": {},
   "source": [
    "## 5D. Model Loading — Llama 3.1 8B (Unsloth Pre-quantized)\n",
    "\n",
    "Load the Unsloth pre-quantized Llama 3.1 8B model with automatic 4-bit quantization and Flash Attention 2 for maximum speed and memory efficiency.\n",
    "\n",
    "**No authentication required** — model downloads from public HuggingFace.\n",
    "\n",
    "Expected load time: ~1-2 minutes on first run (cached thereafter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d67a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print('='*70)\n",
    "print('Loading Llama 3.1 8B with Unsloth (4-bit quantized)')\n",
    "print('='*70)\n",
    "\n",
    "try:\n",
    "    print(f'\\nLoading model: unsloth/Llama-3.1-8B-4bit')\n",
    "    print('(This may take 1-2 minutes on first load...)\\n')\n",
    "    \n",
    "    # Use Unsloth's optimized loader for pre-quantized model\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/Llama-3.1-8B-4bit\",\n",
    "        max_seq_length=512,\n",
    "        dtype=torch.float16,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    \n",
    "    print('✓ Model loaded successfully with Unsloth!')\n",
    "    print(f'Model dtype: {model.dtype}')\n",
    "    print(f'Model device: {next(model.parameters()).device}')\n",
    "    print(f'\\nBenefits:')\n",
    "    print('  - 2x speedup vs standard Hugging Face loading')\n",
    "    print('  - 70% less VRAM usage')\n",
    "    print('  - Flash Attention 2 enabled for inference')\n",
    "    print(f'\\nModel is ready for LoRA fine-tuning.')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'❌ Failed to load model: {e}')\n",
    "    print()\n",
    "    print('Troubleshooting:')\n",
    "    print('1. Ensure unsloth is installed: pip install unsloth')\n",
    "    print('2. Check your internet connection (model downloads ~4GB)')\n",
    "    print('3. Verify you have enough disk space')\n",
    "    print()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c8b99",
   "metadata": {},
   "source": [
    "#### Training arguments, early stopping and checkpointing\n",
    "We'll configure Trainer/TrainingArguments and add an EarlyStoppingCallback to stop when validation loss plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907866ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example TrainingArguments and EarlyStoppingCallback usage (uncomment to run)\n",
    "# from transformers import EarlyStoppingCallback\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     per_device_train_batch_size=per_device_train_batch_size,\n",
    "#     per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "#     evaluation_strategy=evaluation_strategy,\n",
    "#     save_strategy=save_strategy,\n",
    "#     num_train_epochs=num_train_epochs,\n",
    "#     learning_rate=learning_rate,\n",
    "#     logging_steps=logging_steps,\n",
    "#     fp16=fp16,\n",
    "#     gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#     save_total_limit=3,\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "#\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_train,\n",
    "#     eval_dataset=tokenized_val,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "#\n",
    "# trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n",
    "# trainer.train()\n",
    "print('Training arguments and trainer skeleton provided. Run when datasets and model are loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984c4f9",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Analysis\n",
    "We implement: (a) a simple exact-match style metric (normalized whitespace and case-insensitive), (b) generation examples before/after fine-tuning, and (c) a short analysis template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c51f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab setup: mount Drive (optional) and install dependencies\n",
    "# Run this cell in Google Colab (it will skip installs when not in Colab)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    print('Mounting Google Drive...')\n",
    "    drive.mount('/content/drive')\n",
    "    print('Upgrading pip and installing dependencies (this may take a few minutes)')\n",
    "    # Core dependencies used by this notebook; adjust as needed\n",
    "    !pip install -q --upgrade pip\n",
    "    !pip install -q git+https://github.com/unslothai/unsloth.git\n",
    "    !pip install -q transformers datasets accelerate peft bitsandbytes evaluate sentencepiece safetensors\n",
    "    # Optional: install huggingface hub to access gated weights if needed\n",
    "    !pip install -q huggingface_hub\n",
    "    import torch\n",
    "    print('Install finished. PyTorch:', torch.__version__, 'CUDA available:', torch.cuda.is_available())\n",
    "else:\n",
    "    print('Not running in Colab. To use GPU, open this notebook in Google Colab (Runtime -> Change runtime type -> GPU).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0476d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
