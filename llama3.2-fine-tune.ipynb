{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a07f6d73-8bbb-4832-b0cd-a3f30aba009d",
      "metadata": {
        "id": "a07f6d73-8bbb-4832-b0cd-a3f30aba009d"
      },
      "source": [
        "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
        "\n",
        "\n",
        "[![Open llama3.2-fine-tune.ipynb in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/karukan/llamaFinetuning/blob/main/llama3.2-fine-tune.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae6ff1dc-2734-497d-be36-279f2dc76beb",
      "metadata": {
        "id": "ae6ff1dc-2734-497d-be36-279f2dc76beb"
      },
      "source": [
        "# Setup and Installation\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" width=\"20%\" height=\"auto\"/>\n",
        "\n",
        "You **MUST** run this Jupyter notebook at Google Colab.  We will using **unsloth** library. **Unsloth** makes finetuning large language models like Llama-3 2X faster and using 70% less memory with no degradation in accuracy.\n",
        "\n",
        "The following Python code was modified/adapted from Unsloth.\n",
        "\n",
        "### References\n",
        "- [github/unsloth](https://github.com/unslothai/unsloth)\n",
        "- [Unsloth documentation](https://docs.unsloth.ai/)\n",
        "- [huggingface/unsloth](https://huggingface.co/unsloth)\n",
        "- [How to fine-tune an LLM](https://sausheong.com/how-to-fine-tune-an-llm-dc1c7376eb2b)\n",
        "- [4 Open Source Libraries Speed Up LLM Fine-Tuning](https://www.linkedin.com/posts/sumanth077_fine-tuning-massive-llms-used-to-be-painfully-activity-7349701022719623168-zKAY)\n",
        "- [Best frameworks for fine-tuning LLMs in 2025](https://modal.com/blog/fine-tuning-llms)\n",
        "\n",
        "### Versions\n",
        "- unsloth (2025.9.9)\n",
        "- unsloth_zoo (2025.9.12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35db9108-5cd7-4d05-ab39-6ca01fa370d2",
      "metadata": {
        "id": "35db9108-5cd7-4d05-ab39-6ca01fa370d2"
      },
      "source": [
        "### Google Colab\n",
        "- At Google Colab, Go to `Runtime` > `Change runtime type` and select `T4 GPU`. Or if you are lucky, choose `A100 GPU`.\n",
        "- Using T4 GPU is free with limited access time\n",
        "- If you have Colab+ subscription, you can select A100 GPU.\n",
        "\n",
        "<img src=\"https://github.com/koayst-rplesson/C3669C-2025-02/blob/main/Lesson_10/colab-01.png?raw=1\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f63cfe5f-e947-458c-82f6-d6a2ca4fda98",
      "metadata": {
        "id": "f63cfe5f-e947-458c-82f6-d6a2ca4fda98"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install unsloth\n",
        "\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e23847-01ec-41ad-9879-153f503cc11a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16e23847-01ec-41ad-9879-153f503cc11a",
        "outputId": "a886255b-fd94-40f1-dfae-99d0ccc6ede2"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None          # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True   # Use 4bit quantization to reduce memory usage. Can be False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e271b221-9742-42ed-9192-69db51006784",
      "metadata": {
        "id": "e271b221-9742-42ed-9192-69db51006784"
      },
      "outputs": [],
      "source": [
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5405f52d-95a7-4ba2-8d9f-41d7adbb5b5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "adb14ab923834d2086db3c47168b7a82",
            "e29acfbb113948dca1e49f4c567fab35",
            "7aba3f8363984fe797da0819a30ec629",
            "9cbddbb3f31049888db36497a415b369",
            "2493b03e30b04eb48909a1bf19a0b2f7",
            "66b0d8835be64e138f29006628d3f298",
            "71018566bfeb4714b1f3a02c34699891",
            "cec4a2addb154079945bb42b359b996c",
            "8d21069fe17643d29c2e9f42907fd190",
            "8cb522ff83784ea48952f4d79401479b",
            "2c0d92526f0349239369715001d5ffe0",
            "88be6a22b3cd45d5832bcb0b471e04f6",
            "b9979c352a2548919ae861a6d402b59e",
            "b5be4cca0b304e6d8af40ae3ab655342",
            "c521468850a74a6e9235f3a97dd3f423",
            "41910cfd20cd4ae889c7695e10e23c39",
            "e249359adc2941728bb78e1f964e12b6",
            "e7a4b489caa5414aa189ad934a061d1d",
            "bac35c24d2dc4e988b4a50853b190e16",
            "603e75c8b7354287aee480f91eacd336",
            "8a45438f66054ad8b54fd5b3e5dc44ee",
            "332d014865b246f099178bd75033edd3",
            "d84c32a21e37495c8a4b8fe417cd90be",
            "0f4fe196c1394ba8b3fefb0504ff4c2c",
            "4dbe10eccc994b64a3917e62150b27c7",
            "ea39d0f360034e62863da1e85288ffad",
            "f0c7d05275734ea4a4be3f1cf939e367",
            "7e9017344bfb4e818d91fdb944da6c29",
            "87f4cbb688fe4d669acdbfd6ebe2b79d",
            "e71527f1495c488f9b0e43a5f4fe979f",
            "7f39274b26ec4ddfaf96aef87fc762d9",
            "58dd8ddf71044d2bb6e1252ee5f66bc3",
            "4b2c1a16c7f64f7f85ac2cbd3e99417a",
            "3905ed945a774b7fa18a304eca2e55a3",
            "61b7490835b849adbd832ac9d4daf83e",
            "c2122d69feaf4778a4f6ce62e6ad4da3",
            "6fbdf999797a46f28fdf4aca4b35214a",
            "2a99e9804f184666a8b6e9648f9382c7",
            "cfdb41f871c14f34a3b91ca09c721377",
            "2b1e5fb14b1a42278a5b9803dc7a170a",
            "9a965494d0c746b1839bd2ed377931d9",
            "6844f24e2a374ba7ac1b21d1c6794525",
            "2c0b02ceae714d84b64fd3f3116a1586",
            "11a53258b10f44f6833bd5a8d5cb61e3",
            "78d341ca4097457bb0cddd78e717acf7",
            "0ee1076f59c942f796f41d236ce87bb1",
            "2c3355c5a5074f7aa4f63f048ac895b4",
            "021894a7e897468c8f3b064f21f88931",
            "e91c846a0c574312878d012d3ebeef7e",
            "3a04e4cad16648c992d122c62a368902",
            "0207e99eb83743438ffdd82f33c2533d",
            "c803c79bad0a4a6baefe3b2fc5f1e9c5",
            "29c0bd2778d347d38ade89c523ee1d5f",
            "2ca351e016c049f09a2501ac0c4957f5",
            "fc3c7a174e164bd7bb990a9be1a5e8ec"
          ]
        },
        "id": "5405f52d-95a7-4ba2-8d9f-41d7adbb5b5d",
        "outputId": "102da938-f999-4bfc-81aa-ba44f1afdb3e"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LLAMA 3.2 1B INSTRUCT - Gated Model (Requires HuggingFace Authentication)\n",
        "# ============================================================================\n",
        "# \n",
        "# IMPORTANT: Before running this cell:\n",
        "# 1. Accept the license at: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
        "# 2. Create a HF token at: https://huggingface.co/settings/tokens\n",
        "# 3. Authenticate in Colab with the cell below BEFORE loading the model:\n",
        "#\n",
        "#    from huggingface_hub import login\n",
        "#    login()  # Paste your token when prompted\n",
        "#\n",
        "# Then uncomment and run this cell.\n",
        "#\n",
        "# Alternative: Use Option B (Unsloth's pre-quantized Llama 3.1 8B) - no auth needed\n",
        "# ============================================================================\n",
        "\n",
        "from pathlib import Path\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None          # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True   # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# ============================================================================\n",
        "# OPTION A: Llama 3.2 1B Instruct (Recommended - requires HF auth token)\n",
        "# ============================================================================\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Loading: {model_name}\")\n",
        "print(f\"Max sequence length: {max_seq_length}\")\n",
        "print(f\"4-bit quantization: {load_in_4bit}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "try:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = model_name,\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    print(f\"✓ Model loaded successfully!\")\n",
        "    print(f\"  Model: {model_name}\")\n",
        "    print(f\"  Tokenizer vocab size: {len(tokenizer)}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading model: {e}\")\n",
        "    print(f\"\\nIf you see 'Access to model ... is restricted', you need to:\")\n",
        "    print(f\"  1. Accept license: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\")\n",
        "    print(f\"  2. Create HF token: https://huggingface.co/settings/tokens\")\n",
        "    print(f\"  3. Authenticate in Colab:\")\n",
        "    print(f\"     from huggingface_hub import login\")\n",
        "    print(f\"     login()  # Paste token when prompted\")\n",
        "    print(f\"\\nAlternatively, use Option B (Unsloth pre-quantized - no auth needed)\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5639419b",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install jupyter widgets to suppress TqdmWarning (optional but recommended in Colab)\n",
        "!pip install -q ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd145b3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()  # Paste HF token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f7e7239",
      "metadata": {},
      "source": [
        "### Model Loading — Llama 3.2 1B Instruct\n",
        "\n",
        "**This notebook uses Llama 3.2 1B Instruct — a gated model that requires HuggingFace authentication.**\n",
        "\n",
        "#### Quick Setup (4 steps):\n",
        "\n",
        "1. **Install ipywidgets** (suppresses TqdmWarning):\n",
        "   - Run the cell: `!pip install -q ipywidgets`\n",
        "   - This is optional but recommended to clean up warnings\n",
        "\n",
        "2. **Accept the license on HuggingFace:**\n",
        "   - Visit: [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)\n",
        "   - Click \"Accept\" to accept the model license\n",
        "\n",
        "3. **Get your HuggingFace token:**\n",
        "   - Visit: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "   - Create a new token (copy it)\n",
        "\n",
        "4. **Authenticate in Colab:**\n",
        "   ```python\n",
        "   from huggingface_hub import login\n",
        "   login()  # Paste your token when prompted\n",
        "   ```\n",
        "\n",
        "Then run the model loading cell below.\n",
        "\n",
        "**About the TqdmWarning:** If you see warnings like `IProgress not found. Please update jupyter and ipywidgets`, the ipywidgets installation above will suppress them. These warnings don't affect functionality—they just indicate missing progress bar widgets.\n",
        "\n",
        "**Need a faster alternative?** Skip authentication and use **Option B** (Unsloth's pre-quantized Llama 3.1 8B) — no token needed, but larger model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e1e3fe1-3c9b-4d4d-b49f-4007107aaf53",
      "metadata": {
        "id": "8e1e3fe1-3c9b-4d4d-b49f-4007107aaf53"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02bc4fb5-d612-4851-962c-fb95261f4c19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02bc4fb5-d612-4851-962c-fb95261f4c19",
        "outputId": "82aefbed-4db4-4d21-b7d6-1e2b0a0b990e"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "\n",
        "    target_modules = [\"q_proj\",\n",
        "                      \"k_proj\",\n",
        "                      \"v_proj\",\n",
        "                      \"o_proj\",\n",
        "                      \"gate_proj\",\n",
        "                      \"up_proj\",\n",
        "                      \"down_proj\",],\n",
        "\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcb6982f-4090-4995-a95e-cdcc47c8ce36",
      "metadata": {
        "id": "dcb6982f-4090-4995-a95e-cdcc47c8ce36"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Preparation\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e822a697-6779-4ed3-938c-98174cf120ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "29383225b65543f1ae81fe53a6892098",
            "e986d1ceb7d84e0e99f9e3346aa9a67c",
            "4e4a58027a354c09a7b8b3d4f9c5469e",
            "6fcd6041cd1f436ea4486d1f159ba6fc",
            "ac6029dde7db4a259d75800838248b54",
            "7fc9ff4506694f3fa06b8d5178edca3e",
            "c842c714bdd34466a333c428517ffa54",
            "fa048ded84ee47eaa2503e36e5e16ab7",
            "f59e87e4c5a04c6090ee3a7fce549807",
            "daf8ddfa3f2e47dfa25caad7a4ad5e74",
            "3670060316ea4feb98c13ffe27a0a816",
            "88a63b0bcbd342369661639ab8fb449c",
            "6e68a49ba8a1482c98dcb3cdcca9ee33",
            "83457daf51bb4b0b89c67ff9dbbc146f",
            "1b452faf65224462a37837a23ab56699",
            "4235744cd89f4f889b3951939ba90b76",
            "460d58bd25964d0683439abd5f07ef42",
            "9a7ed5e0288144b19222be3dfe44b88d",
            "d208f1f5863e44648bdc805ea25f4bd0",
            "80a131ce7e5848de992ccd390156fc24",
            "3fb80abded9b4bef95dadc89bf76c434",
            "3f3e33dac1874dd0b58fd3ac36202955",
            "ef5fc187e36843adb0ba2aeba38fc2e8",
            "5c0ad0df90e54d0c9e7872d08d338958",
            "3a70321823a04c119db4d045f4982492",
            "597d1502f02143eba5852f50c5900d37",
            "09ff34966bfc48c194b13a7e0f0cfa59",
            "ebc9e784bcd24ce587e0aa0adde8a650",
            "e9699f0e23964eec8de1b83f5c044c5d",
            "012dbcf161634e40acb80ed512cffa53",
            "bfaf8274a8e245f295c367f896225c5e",
            "93d292271d0a4d0ba8d87654964db4c4",
            "07c33fb9fbc04671a1cc03eb82a0b0d5",
            "8bccce1514f34b8a8a1b857f28068c77",
            "9d3ab27cc2ae4b4cba10e7d88f7c1428",
            "bffd06b08aa1483481ab25fbc095643a",
            "caaea18367624635b0a8ab29e0b2d75b",
            "021a34e3a70b4de2aa410a1777fc4755",
            "b24d0be4f89947ad8613ee59d561ba29",
            "9f516bc6d13345d5ab83b1a8f723eb69",
            "eb2ad0fdda714f4487b06aaf7cc3a491",
            "695edebff1dd4342b59f5a418cc384cb",
            "a9d9c6658173485cb86f0d1743e88249",
            "61c1cf627c4342009509098970901340"
          ]
        },
        "id": "e822a697-6779-4ed3-938c-98174cf120ac",
        "outputId": "b30e12d3-6034-4471-9745-d71bcedf2a3d"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903ac708-f84e-4595-8b65-c2c5baf3eef0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "903ac708-f84e-4595-8b65-c2c5baf3eef0",
        "outputId": "fc06e303-faa2-438c-a0d6-f53f21305a42"
      },
      "outputs": [],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3325f3d-844d-4d8b-ae43-069b59f5d904",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3325f3d-844d-4d8b-ae43-069b59f5d904",
        "outputId": "4ea75530-b8ea-4e0b-fe5f-77219c58e004"
      },
      "outputs": [],
      "source": [
        "print(f\"Output:\\n{dataset[0]['output']}\\n\")\n",
        "print(f\"Input:\\n{dataset[0]['input']}\\n\")\n",
        "print(f\"Instruction:\\n{dataset[0]['instruction']}\\n\")\n",
        "print(f\"Text:\\n{dataset[0]['text']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a15d7ee-cea0-4272-b17d-d834126277ef",
      "metadata": {
        "id": "7a15d7ee-cea0-4272-b17d-d834126277ef"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa217556-7984-48cb-b122-5208878bb6ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "af90c8dca4a24afbae51b19057c55477",
            "0eef07c64f4741678e0587f9dc62454b",
            "2f4e251f190b472baa134d2f6111e115",
            "111b8e087d1e41e4a25abe56e289a26e",
            "8240db8129e841dab607c0930351571b",
            "cb8e561818c24572ac151a9e7f1d01a8",
            "1628dbeb21a94468ac1b431d2d8ffd01",
            "9e42f707d0bb4092a2bf409e156f3651",
            "6d5cc035ad8745718e7ac5606d640bf2",
            "c79f20bc342045af91f649cce5ace93f",
            "dd4cf1082767445c9fa7dc4b63f4bdbe"
          ]
        },
        "id": "aa217556-7984-48cb-b122-5208878bb6ca",
        "outputId": "e5b3c6ba-4863-4922-9795-76e3b3d5572b"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c3f0847-7d17-46b3-b96b-f454b6c1f1a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c3f0847-7d17-46b3-b96b-f454b6c1f1a0",
        "outputId": "4f6fdc35-ab29-4aa7-bce5-164a8ba43a04"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebbb4f94-8023-452e-98b5-2098cc51bc36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ebbb4f94-8023-452e-98b5-2098cc51bc36",
        "outputId": "535c6c1e-a6f7-47d6-cfb4-71d0fc5ce4a4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ee1f4a-98d6-493e-a437-bc0dcffaa18c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0ee1f4a-98d6-493e-a437-bc0dcffaa18c",
        "outputId": "2a19caca-8e27-4a3a-fa4d-acd028bc0520"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6118073-c1e6-465a-a116-cc39b92f4e3c",
      "metadata": {
        "id": "c6118073-c1e6-465a-a116-cc39b92f4e3c"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b4c5d5-5ee0-45bb-9865-468a8b4dd8c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27b4c5d5-5ee0-45bb-9865-468a8b4dd8c8",
        "outputId": "7de8ca25-0d99-4f79-c886-fa0c49f40af7"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "451dc78c-ed93-4979-9b4c-9bfce9ee6610",
      "metadata": {
        "id": "451dc78c-ed93-4979-9b4c-9bfce9ee6610"
      },
      "source": [
        "You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "911e502e-1758-46ee-9422-8a0fffdc2ce2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "911e502e-1758-46ee-9422-8a0fffdc2ce2",
        "outputId": "c4a9e0c5-7ad1-474e-cf54-8be10616d1d0"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # instruction\n",
        "        \"1, 1, 2, 3, 5, 8\", # input\n",
        "        \"\", # output/response - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4193f44d-98da-4825-b263-e260d9519e83",
      "metadata": {
        "id": "4193f44d-98da-4825-b263-e260d9519e83"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce2bc6f-c82d-4847-b2b9-179987495256",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ce2bc6f-c82d-4847-b2b9-179987495256",
        "outputId": "2e4c5940-3d67-43d5-8d99-894f72ee4164"
      },
      "outputs": [],
      "source": [
        "# connect to google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f456cc49-9380-4dd8-b650-a3ba2d1773c6",
      "metadata": {
        "id": "f456cc49-9380-4dd8-b650-a3ba2d1773c6"
      },
      "outputs": [],
      "source": [
        "path_to_saved_model = \"/content/drive/MyDrive/Colab Notebooks/c3669c/L06/lora_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71435773-e19e-45b3-bd4e-8fee9a94359f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71435773-e19e-45b3-bd4e-8fee9a94359f",
        "outputId": "6d9928bc-cc25-4d36-f841-88d1dc74fbe2"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(path_to_saved_model) # Local saving\n",
        "tokenizer.save_pretrained(path_to_saved_model)\n",
        "\n",
        "# you can push to huggingface if you have an account\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe03aee-35d3-4249-9aca-38a2ada511da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fe03aee-35d3-4249-9aca-38a2ada511da",
        "outputId": "9c3b4f74-ef65-4580-fe57-b3480692863e"
      },
      "outputs": [],
      "source": [
        "# check the files are save to directory lora_model\n",
        "\n",
        "!ls -al \"/content/drive/MyDrive/Colab Notebooks/c3669c/L06/lora_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8afc5ee-1c94-4e01-84f1-63a932e8fd9d",
      "metadata": {
        "id": "c8afc5ee-1c94-4e01-84f1-63a932e8fd9d"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2c67e5-d0cc-407d-8ece-4fcc50fbe617",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae2c67e5-d0cc-407d-8ece-4fcc50fbe617",
        "outputId": "8cc84ff3-9820-4e37-dcf5-f6849bb29acb"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = path_to_saved_model, # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f6b1c49-0668-4787-89e7-ec7396aead14",
      "metadata": {
        "id": "6f6b1c49-0668-4787-89e7-ec7396aead14"
      },
      "outputs": [],
      "source": [
        "# alpaca_prompt = You MUST copy from above!\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4179207-85ab-41ba-8029-0b11ed3252ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4179207-85ab-41ba-8029-0b11ed3252ce",
        "outputId": "3d2a7b42-77b4-4077-cbad-34dfd6a06c94"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"What is a famous tall tower in Paris?\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "808bec63-ca41-409a-88a5-e49ac0740336",
      "metadata": {
        "id": "808bec63-ca41-409a-88a5-e49ac0740336"
      },
      "source": [
        "### Cleanup\n",
        "\n",
        "If you don't need the models anymore, remember to go to `path_to_saved_model` and delete the directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d03441",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "unsloth311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
